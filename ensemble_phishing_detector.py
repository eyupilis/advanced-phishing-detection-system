#!/usr/bin/env python3
"""
4-Model Ensemble Phishing Detector
Modeller:
1. Mega Phishing Detector (20K URLs, 96 features)
2. Cybersecurity VirusTotal Analyzer (4K domains, 17 features)
3. Advanced URL Feature Analyzer (549K URLs, 35 features)
4. Website Feature Detector (11K websites, 31 features)
"""

import pandas as pd
import numpy as np
import joblib
import json
import re
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# √ñnceki modellerden feature extractor'larƒ± import et
from feature_extractor import FeatureExtractor, RuleBasedAnalyzer

class EnsemblePhishingDetector:
    """
    √áoklu Model Ensemble Sistemi
    
    Birden fazla veri seti ve algoritmadan eƒüitilmi≈ü modelleri birle≈ütirerek
    daha g√ºvenilir phishing tespiti yapar.
    
    Features:
    - Multi-model voting mechanism
    - Weighted predictions
    - Individual model transparency
    - Adaptive learning from feedback
    - Confidence scoring
    """
    
    def __init__(self):
        self.models = {}
        self.model_weights = {}
        self.model_performance = {}
        self.feature_extractor = FeatureExtractor()
        self.rule_analyzer = RuleBasedAnalyzer()
        
        # Model metadata
        self.model_info = {
            'phishing_model': {
                'name': 'Mega Phishing Detector',
                'dataset': '20K URLs with 96 features',
                'algorithm': 'Random Forest',
                'speciality': 'General URL pattern analysis'
            },
            'cybersecurity_model': {
                'name': 'Cybersecurity VirusTotal Analyzer',
                'dataset': '4K domains with VirusTotal data',
                'algorithm': 'CatBoost',
                'speciality': 'Security engine analysis'
            },
            'phishing_urls_model': {
                'name': 'Advanced URL Feature Analyzer',
                'dataset': '549K URLs with 35 URL features',
                'algorithm': 'Random Forest',
                'speciality': 'Comprehensive URL structure analysis'
            },
            'website_model': {
                'name': 'Website Feature Detector',
                'dataset': '11K websites with 31 features',
                'algorithm': 'Machine Learning',
                'speciality': 'Website behavior analysis'
            }
        }
        
        # Feedback storage
        self.feedback_history = []
        
        # Load models
        self.load_all_models()
    
    def load_all_models(self):
        """T√ºm eƒüitilmi≈ü modelleri y√ºkle"""
        
        print("üîÑ Ensemble modelleri y√ºkleniyor...")
        
        # Model 1: Mega Phishing Detector
        try:
            self.models['phishing_model'] = joblib.load('best_phishing_model.pkl')
            self.model_info['phishing_model']['selected_features'] = joblib.load('selected_features.pkl')
            
            # Performance bilgilerini y√ºkle
            try:
                with open('model_info.pkl', 'rb') as f:
                    model_info = joblib.load(f)
                    self.model_performance['phishing_model'] = model_info.get('performance', {})
            except:
                self.model_performance['phishing_model'] = {'accuracy': 0.9991, 'auc_score': 0.9995}
            
            print("   ‚úÖ Phishing Model y√ºklendi")
            
        except Exception as e:
            print(f"   ‚ùå Phishing Model y√ºklenemedi: {e}")
        
        # Model 2: Cybersecurity Model
        try:
            self.models['cybersecurity_model'] = joblib.load('cybersecurity_model_catboost.pkl')
            
            # Model info y√ºkle
            try:
                cybersecurity_info = joblib.load('cybersecurity_model_catboost_info.pkl')
                self.model_performance['cybersecurity_model'] = cybersecurity_info.get('performance', {})
                self.model_info['cybersecurity_model']['selected_features'] = cybersecurity_info.get('selected_features', [])
            except:
                self.model_performance['cybersecurity_model'] = {'accuracy': 0.9964, 'auc_score': 1.0000}
            
            print("   ‚úÖ Cybersecurity Model y√ºklendi")
            
        except Exception as e:
            print(f"   ‚ùå Cybersecurity Model y√ºklenemedi: {e}")
        
        # Model 3: Advanced URL Feature Model
        try:
            self.models['phishing_urls_model'] = joblib.load('phishing_urls_model_best.pkl')
            
            # Model info ve feature extractor components y√ºkle
            try:
                urls_info = joblib.load('phishing_urls_model_best_info.pkl')
                self.models['phishing_urls_model_scaler'] = joblib.load('phishing_urls_model_best_scaler.pkl')
                self.models['phishing_urls_model_label_encoder'] = joblib.load('phishing_urls_model_best_label_encoder.pkl')
                self.models['phishing_urls_model_feature_selector'] = joblib.load('phishing_urls_model_best_feature_selector.pkl')
                
                self.model_performance['phishing_urls_model'] = urls_info.get('model_performance', {}).get('RandomForest', {})
                self.model_info['phishing_urls_model']['feature_names'] = urls_info.get('feature_names', [])
                
                # Feature extraction pipeline ekle
                from phishing_urls_model_pipeline import PhishingURLsDetectorPipeline
                self.models['phishing_urls_extractor'] = PhishingURLsDetectorPipeline()
                
            except Exception as sub_e:
                print(f"     ‚ö†Ô∏è URL Model bile≈üenleri y√ºklenirken hata: {sub_e}")
                self.model_performance['phishing_urls_model'] = {'accuracy': 0.9125, 'auc_score': 0.9564}
            
            print("   ‚úÖ Advanced URL Feature Model y√ºklendi")
            
        except Exception as e:
            print(f"   ‚ùå Advanced URL Feature Model y√ºklenemedi: {e}")
        
        # Model 4: Website Feature Detector
        try:
            self.models['website_model'] = joblib.load('phishing_website_model_best.pkl')
            
            # Model info ve feature extractor components y√ºkle
            try:
                website_info = joblib.load('phishing_website_model_best_info.pkl')
                self.model_info['website_model']['selected_features'] = website_info.get('selected_features', [])
                self.model_info['website_model']['feature_selector'] = joblib.load('phishing_website_model_best_feature_selector.pkl')
                self.model_info['website_model']['info'] = website_info.get('info', {})
                
                self.model_performance['website_model'] = website_info.get('model_performance', {})
                
                # Feature extraction pipeline ekle
                from phishing_website_model_pipeline import PhishingWebsiteDetectorPipeline
                self.models['website_extractor'] = PhishingWebsiteDetectorPipeline()
                
            except Exception as sub_e:
                print(f"     ‚ö†Ô∏è Website Model bile≈üenleri y√ºklenirken hata: {sub_e}")
                self.model_performance['website_model'] = {'accuracy': 0.9656, 'auc_score': 0.9995}
            
            print("   ‚úÖ Website Feature Model y√ºklendi")
            
        except Exception as e:
            print(f"   ‚ùå Website Feature Model y√ºklenemedi: {e}")
        
        # 5. Cryptocurrency Scam Model - Ge√ßici olarak devre dƒ±≈üƒ± (√∂zellik uyumsuzluƒüu)
        try:
            print("   ‚ö†Ô∏è Cryptocurrency Scam Model ge√ßici olarak devre dƒ±≈üƒ± bƒ±rakƒ±ldƒ± (√∂zellik uyumsuzluƒüu)")
            self.models['crypto_scam_model'] = None
        except Exception as e:
            print(f"   ‚ùå Cryptocurrency Scam Model y√ºklenemedi: {e}")
            self.models['crypto_scam_model'] = None
        
        # 6. Link Phishing Model
        try:
            print("   üîÑ Link Phishing Model y√ºkleniyor...")
            from link_phishing_model_pipeline import LinkPhishingDetectorPipeline
            link_pipeline = LinkPhishingDetectorPipeline()
            if link_pipeline.load_model():
                self.models['link_phishing_model'] = link_pipeline.model
                self.model_info['link_phishing_model'] = {
                    'type': 'LinkPhishingDetector',
                    'features': 89,
                    'accuracy': 0.9892,
                    'specialization': 'Advanced link analysis',
                    'label_encoder': link_pipeline.label_encoder,
                    'scaler': link_pipeline.scaler,
                    'feature_selector': link_pipeline.feature_selector,
                    'selected_features': link_pipeline.selected_features,
                    'feature_names': link_pipeline.feature_names,
                    'pipeline': link_pipeline
                }
                self.model_performance['link_phishing_model'] = {'accuracy': 0.9892, 'auc_score': 0.9992}
                print(f"   ‚úÖ Link Phishing Model y√ºklendi")
            else:
                self.models['link_phishing_model'] = None
                print(f"   ‚ö†Ô∏è Link Phishing Model bulunamadƒ±")
        except Exception as e:
            print(f"   ‚ùå Link Phishing Model y√ºklenemedi: {e}")
            self.models['link_phishing_model'] = None
        
        # 7. Malicious URLs Model (NEW!)
        try:
            print("   üîÑ Malicious URLs Model y√ºkleniyor...")
            from malicious_urls_model_pipeline import MaliciousURLsDetectorPipeline
            malicious_pipeline = MaliciousURLsDetectorPipeline()
            
            # Model dosyalarƒ±nƒ± load et
            try:
                malicious_pipeline.model = joblib.load('malicious_urls_model_best.pkl')
                malicious_pipeline.scaler = joblib.load('malicious_urls_model_best_scaler.pkl')
                malicious_pipeline.feature_selector = joblib.load('malicious_urls_model_best_feature_selector.pkl')
                malicious_pipeline.selected_features = joblib.load('malicious_urls_model_best_selected_features.pkl')
                malicious_pipeline.feature_names = joblib.load('malicious_urls_model_best_feature_names.pkl')
                
                model_info = joblib.load('malicious_urls_model_best_info.pkl')
                
                self.models['malicious_urls_model'] = malicious_pipeline.model
                self.model_info['malicious_urls_model'] = {
                    'type': 'MaliciousURLsDetector',
                    'features': model_info['feature_count'],
                    'accuracy': model_info['accuracy'],
                    'auc_score': model_info['auc_score'],
                    'specialization': 'Multi-threat URL detection (phishing, malware, defacement)',
                    'scaler': malicious_pipeline.scaler,
                    'feature_selector': malicious_pipeline.feature_selector,
                    'selected_features': malicious_pipeline.selected_features,
                    'feature_names': malicious_pipeline.feature_names,
                    'pipeline': malicious_pipeline
                }
                self.model_performance['malicious_urls_model'] = {
                    'accuracy': model_info['accuracy'], 
                    'auc_score': model_info['auc_score']
                }
                print(f"   ‚úÖ Malicious URLs Model y√ºklendi (AUC: {model_info['auc_score']:.4f})")
            except Exception as load_e:
                print(f"     ‚ùå Model dosyalarƒ± y√ºklenemedi: {load_e}")
                self.models['malicious_urls_model'] = None
                
        except Exception as e:
            print(f"   ‚ùå Malicious URLs Model y√ºklenemedi: {e}")
            self.models['malicious_urls_model'] = None
        
        # 6. Link Phishing Detection Model
        try:
            print("   üîÑ Link Phishing Detection Model y√ºkleniyor...")
            from link_phishing_model_pipeline import LinkPhishingDetectorPipeline
            link_pipeline = LinkPhishingDetectorPipeline()
            if link_pipeline.load_model():
                self.models['link_phishing_model'] = link_pipeline.model
                self.model_info['link_phishing_model'] = {
                    'name': 'Link Phishing Detection Model',
                    'dataset': '19K URLs with 87 comprehensive features',
                    'algorithm': 'XGBoost',
                    'speciality': 'Comprehensive link analysis with 50 selected features',
                    'type': 'LinkPhishingDetector',
                    'features': 50,
                    'accuracy': 0.9892,
                    'auc_score': 0.9992,
                    'specialization': 'Advanced URL pattern and content analysis',
                    'label_encoder': link_pipeline.label_encoder,
                    'scaler': link_pipeline.scaler,
                    'feature_selector': link_pipeline.feature_selector,
                    'selected_features': link_pipeline.selected_features,
                    'feature_names': link_pipeline.feature_names,
                    'pipeline': link_pipeline
                }
                self.model_performance['link_phishing_model'] = {'accuracy': 0.9892, 'auc_score': 0.9992}
                print(f"   ‚úÖ Link Phishing Detection Model y√ºklendi ({link_pipeline.model.__class__.__name__})")
            else:
                self.models['link_phishing_model'] = None
                print(f"   ‚ö†Ô∏è Link Phishing Detection Model bulunamadƒ±")
        except Exception as e:
            print(f"   ‚ùå Link Phishing Detection Model y√ºklenemedi: {e}")
            self.models['link_phishing_model'] = None
        
        # Initialize model weights based on performance
        self.initialize_weights()
        
        successful_models = len([m for m in self.models.values() if m is not None])
        print(f"‚úÖ {successful_models} model ba≈üarƒ±yla y√ºklendi (toplam {len(self.models)} model tanƒ±mlƒ±)")
    
    def initialize_weights(self):
        """Model aƒüƒ±rlƒ±klarƒ±nƒ± performansa g√∂re ba≈ülat - √ñncelikli modeller aƒüƒ±rlƒ±klandƒ±rƒ±ldƒ±"""
        
        # √ñncelikli modeller - daha y√ºksek aƒüƒ±rlƒ±k
        priority_models = {
            'phishing_model': 2.5,      # MEGA PHISHING - En y√ºksek aƒüƒ±rlƒ±k
            'cybersecurity_model': 2.3,  # CYBERSECURITY - ƒ∞kinci y√ºksek
            'link_phishing_model': 2.0   # LINK SCANNER - √ú√ß√ºnc√º y√ºksek
        }
        
        if not self.model_performance:
            # Default weights with priority
            self.model_weights = {}
            for model_name in self.models.keys():
                if model_name in priority_models:
                    self.model_weights[model_name] = priority_models[model_name]
                else:
                    self.model_weights[model_name] = 1.0
            return
        
        total_performance = 0
        model_scores = {}
        
        for model_name, performance in self.model_performance.items():
            # AUC Score + Accuracy kombinasyonu
            base_score = (performance.get('auc_score', 0.5) + performance.get('accuracy', 0.5)) / 2
            
            # √ñncelikli modellere bonus aƒüƒ±rlƒ±k
            if model_name in priority_models:
                priority_multiplier = priority_models[model_name]
                final_score = base_score * priority_multiplier
                print(f"   üéØ √ñncelikli model {model_name}: base={base_score:.3f} √ó {priority_multiplier} = {final_score:.3f}")
            else:
                final_score = base_score
            
            model_scores[model_name] = final_score
            total_performance += final_score
        
        # Normalize weights
        for model_name, score in model_scores.items():
            self.model_weights[model_name] = score / total_performance if total_performance > 0 else 1.0
        
        print("üéØ G√úNCEL Model aƒüƒ±rlƒ±klarƒ± (√ñncelikli modeller aƒüƒ±rlƒ±klandƒ±rƒ±ldƒ±):")
        for model_name, weight in self.model_weights.items():
            priority_marker = " üî•" if model_name in priority_models else ""
            print(f"   {model_name}: {weight:.3f}{priority_marker}")
    
    def extract_features_for_model(self, url: str, model_name: str):
        """Belirli model i√ßin √∂zellik √ßƒ±karƒ±mƒ±"""
        
        try:
            if model_name == 'phishing_model':
                # Mega phishing dataset √∂zellikleri
                basic_features = self.feature_extractor.extract_features(url)
                
                # Selected features'a g√∂re sƒ±rayla veri hazƒ±rla
                if 'selected_features' in self.model_info[model_name]:
                    selected_features = self.model_info[model_name]['selected_features']
                    # Feature values'ƒ± doƒüru sƒ±rada hazƒ±rla
                    feature_values = []
                    for feature_name in selected_features:
                        feature_values.append(basic_features.get(feature_name, 0))
                    
                    # DataFrame olu≈üturmak yerine doƒürudan array d√∂nd√ºr
                    return np.array([feature_values])
                else:
                    # T√ºm featureslarƒ± kullan
                    feature_values = list(basic_features.values())
                    return np.array([feature_values])
            
            elif model_name == 'cybersecurity_model':
                # Cybersecurity model i√ßin √∂zellik √ßƒ±karƒ±mƒ±
                # Bu model VirusTotal benzeri analiz sonu√ßlarƒ± bekliyor
                # Simulated features (ger√ßek uygulamada VirusTotal API'den gelecek)
                
                features = [
                    int('.onion' in url.lower()),  # is_onion
                    self._extract_tld(url),  # tld
                    0,  # categories_sophos
                    0,  # categories_alpha_mountain
                    self._calculate_reputation(url),  # reputation
                    0,  # number_of_tags
                    60,  # last_analysis_stats_harmles
                    5,  # last_analysis_stats_malicious
                    0,  # last_analysis_stats_suspicious
                    25,  # last_analysis_stats_undetected
                    0,  # total_votes_harmless
                    0,  # total_votes_malicious
                    5/61,  # malicious_harmless_ratio
                    90,  # total_analysis_score
                    2  # reputation_category
                ]
                
                return np.array([features])
            
            elif model_name == 'phishing_urls_model':
                # Advanced URL Feature Model i√ßin √∂zellik √ßƒ±karƒ±mƒ±
                try:
                    if 'phishing_urls_extractor' in self.models:
                        extractor = self.models['phishing_urls_extractor']
                        features = extractor.extract_url_features(url)
                        
                        # Feature names ile sƒ±ralƒ± array olu≈ütur
                        if 'feature_names' in self.model_info[model_name]:
                            feature_names = self.model_info[model_name]['feature_names']
                            feature_values = [features.get(name, 0) for name in feature_names]
                            feature_array = np.array([feature_values])
                        else:
                            # Fallback: feature values'larƒ± doƒürudan al
                            feature_array = np.array([list(features.values())])
                        
                        # Feature selection uygula
                        if 'phishing_urls_model_feature_selector' in self.models:
                            feature_selector = self.models['phishing_urls_model_feature_selector']
                            feature_array = feature_selector.transform(feature_array)
                        
                        return feature_array
                    else:
                        # Extractor yoksa basit √∂zellik √ßƒ±karƒ±mƒ±
                        feature_array = self._extract_basic_url_features(url)
                        
                        # Feature selection uygula
                        if 'phishing_urls_model_feature_selector' in self.models:
                            feature_selector = self.models['phishing_urls_model_feature_selector']
                            feature_array = feature_selector.transform(feature_array)
                        
                        return feature_array
                except Exception as sub_e:
                    print(f"     URL feature extraction hatasƒ±: {sub_e}")
                    return self._extract_basic_url_features(url)
            
            elif model_name == 'website_model':
                # Website Feature Detector i√ßin √∂zellik √ßƒ±karƒ±mƒ±
                try:
                    if 'website_extractor' in self.models:
                        extractor = self.models['website_extractor']
                        features = extractor.extract_website_features(url)
                        
                        # Class s√ºtununu kaldƒ±r
                        if 'class' in features:
                            del features['class']
                        
                        # DataFrame olu≈ütur
                        import pandas as pd
                        features_df = pd.DataFrame([features])
                        
                        # Feature engineering uygula
                        features_engineered = extractor.feature_engineering(features_df)
                        
                        # Selected features'a g√∂re sƒ±rala
                        if 'selected_features' in self.model_info[model_name]:
                            selected_features = self.model_info[model_name]['selected_features']
                            # Sadece mevcut √∂zellikleri se√ß
                            available_features = [col for col in selected_features if col in features_engineered.columns]
                            if available_features:
                                features_selected = features_engineered[available_features]
                                return features_selected.values
                        
                        # Feature selection uygula
                        if 'feature_selector' in self.model_info[model_name]:
                            feature_selector = self.model_info[model_name]['feature_selector']
                            return feature_selector.transform(features_engineered)
                        
                        return features_engineered.values
                    else:
                        # Fallback
                        return self._extract_basic_website_features(url)
                        
                except Exception as sub_e:
                    print(f"‚ùå Website model √∂zellik √ßƒ±karƒ±mƒ±nda hata: {sub_e}")
                    return self._extract_basic_website_features(url)
            
            elif model_name == 'crypto_scam_model':
                # Cryptocurrency Scam Model i√ßin √∂zellik √ßƒ±karƒ±mƒ±
                try:
                    if 'pipeline' in self.model_info[model_name]:
                        # Pipeline √ºzerinden TAM feature extraction (URL + name + desc + addresses)
                        pipeline = self.model_info[model_name]['pipeline']
                        
                        # Full dataset formatƒ±nda DataFrame olu≈ütur
                        import pandas as pd
                        from urllib.parse import urlparse
                        parsed_url = urlparse(url)
                        temp_df = pd.DataFrame({
                            'url': [url],
                            'name': [parsed_url.netloc],  # Domain name
                            'description': [''],  # Bo≈ü description
                            'addresses': ['']  # Bo≈ü addresses
                        })
                        
                        # Pipeline'ƒ±n create_features metodunu kullan
                        features_df = pipeline.create_features(temp_df)
                        features_array = features_df.values
                        
                        # Scaler uygula
                        if 'scaler' in self.model_info[model_name]:
                            scaler = self.model_info[model_name]['scaler']
                            features_array = scaler.transform(features_array)
                        
                        # Feature selector uygula
                        if 'feature_selector' in self.model_info[model_name]:
                            feature_selector = self.model_info[model_name]['feature_selector']
                            features_array = feature_selector.transform(features_array)
                        
                        return features_array
                    else:
                        # Pipeline yoksa fallback
                        return self._extract_basic_crypto_features_30(url)
                        
                except Exception as sub_e:
                    print(f"‚ùå Crypto model √∂zellik √ßƒ±karƒ±mƒ±nda hata: {sub_e}")
                    # Fallback: 30 √∂zellik ile default deƒüerler
                    return np.array([[0] * 30])
            
            elif model_name == 'link_phishing_model':
                # Link Phishing Detection Model i√ßin √∂zellik √ßƒ±karƒ±mƒ±
                try:
                    if 'pipeline' in self.model_info[model_name]:
                        # Pipeline √ºzerinden feature extraction
                        pipeline = self.model_info[model_name]['pipeline']
                        
                        # URL'yi dataset formatƒ±nda hazƒ±rla  
                        import pandas as pd
                        temp_df = pd.DataFrame({'url': [url]})
                        
                        # Feature engineering
                        features = self._extract_basic_link_phishing_features(url)
                        
                        # Feature selection uygula
                        if 'feature_selector' in self.model_info[model_name]:
                            feature_selector = self.model_info[model_name]['feature_selector']
                            features = feature_selector.transform(features)
                        
                        return features
                    else:
                        # Pipeline yoksa basit feature extraction
                        return self._extract_basic_link_phishing_features(url)
                        
                except Exception as sub_e:
                    print(f"‚ùå Link phishing model √∂zellik √ßƒ±karƒ±mƒ±nda hata: {sub_e}")
                    return self._extract_basic_link_phishing_features(url)
            
            elif model_name == 'malicious_urls_model':
                # Malicious URLs Model i√ßin √∂zellik √ßƒ±karƒ±mƒ± (7. Model)
                try:
                    # Her zaman fallback basit √∂zellik √ßƒ±karƒ±mƒ± kullan (43 features)
                    features = self._extract_basic_malicious_urls_features(url)
                    
                    # Model info varsa scaler ve feature selector uygula
                    if 'scaler' in self.model_info.get(model_name, {}):
                        scaler = self.model_info[model_name]['scaler']
                        features = scaler.transform(features)
                    
                    if 'feature_selector' in self.model_info.get(model_name, {}):
                        feature_selector = self.model_info[model_name]['feature_selector']
                        features = feature_selector.transform(features)
                    
                    return features
                        
                except Exception as sub_e:
                    print(f"‚ùå Malicious URLs model √∂zellik √ßƒ±karƒ±mƒ±nda hata: {sub_e}")
                    return self._extract_basic_malicious_urls_features(url)
            
        except Exception as e:
            print(f"‚ùå {model_name} i√ßin √∂zellik √ßƒ±karƒ±mƒ±nda hata: {e}")
            return None
        
        return None
    
    def _extract_tld(self, url: str) -> int:
        """TLD √ßƒ±kar ve encode et"""
        try:
            from urllib.parse import urlparse
            domain = urlparse(url).netloc
            if '.' in domain:
                tld = domain.split('.')[-1]
                # Common TLD'leri encode et
                tld_mapping = {'com': 1, 'org': 2, 'net': 3, 'edu': 4, 'gov': 5}
                return tld_mapping.get(tld, 0)
        except:
            pass
        return 0
    
    def _extract_basic_website_features(self, url: str):
        """Website model i√ßin basit √∂zellik √ßƒ±karƒ±mƒ± (fallback)"""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            
                                    # Website model i√ßin 35 √∂zellik (model bu kadar bekliyor)
            features = [
                int(parsed.scheme == 'https'),  # having_IP_Address
                len(url),  # URL_Length
                int(len(url) < 50),  # Shortining_Service  
                int('@' in url),  # having_At_Symbol
                int('//' in parsed.path),  # double_slash_redirecting
                int('-' in parsed.netloc),  # Prefix_Suffix
                parsed.netloc.count('.') - 1 if '.' in parsed.netloc else 0,  # having_Sub_Domain
                0,  # SSLfinal_State (0=safe default)
                len(parsed.netloc) if parsed.netloc else 0,  # Domain_registeration_length
                0,  # Favicon (0=safe default)
                int(parsed.port is not None),  # port
                int('https' in url.lower()),  # HTTPS_token
                len([x for x in parsed.path.split('/') if x]),  # Request_URL
                0,  # URL_of_Anchor (0=safe default)
                0,  # Links_in_tags (0=safe default)
                0,  # SFH (0=safe default)
                0,  # Submitting_to_email (0=safe default)
                int(bool(re.search(r'redirect|forward', url.lower()))),  # Abnormal_URL
                0,  # Redirect (0=safe default)
                0,  # on_mouseover (0=safe default)
                0,  # RightClick (0=safe default)
                0,  # popUpWidnow (0=safe default)
                0,  # Iframe (0=safe default)
                0,  # age_of_domain (0=safe default)
                0,  # DNSRecord (0=safe default)
                # Ek √∂zellikler (35'e tamamlamak i√ßin)
                int(parsed.scheme == 'http'),  # http_scheme
                int('www' in parsed.netloc),  # has_www
                len(parsed.path),  # path_length
                int(parsed.netloc.replace('.', '').isdigit()),  # is_ip
                parsed.netloc.count('-'),  # hyphen_count
                len(parsed.query) if parsed.query else 0,  # query_length
                int(len(parsed.netloc.split('.')) > 3),  # deep_subdomain
                0,  # additional_feature_32
                0,  # additional_feature_33
                0,  # additional_feature_34
                0   # additional_feature_35
            ]
            
            return np.array([features])
        except:
            # Fallback: 35 √∂zellik i√ßin safe defaults
            return np.array([[0] * 35])

    def _extract_basic_crypto_features_30(self, url: str):
        """Cryptocurrency model i√ßin 30 √∂zellik √ßƒ±karƒ±mƒ± (model beklentisine uygun)"""
        try:
            from urllib.parse import urlparse
            import math
            
            parsed = urlparse(url)
            domain = parsed.netloc
            path = parsed.path
            query = parsed.query
            
            # Model sadece 30 √∂zellik bekliyor
            features = []
            
            # 1-10: Temel URL √∂zellikleri
            features.append(len(url))  # url_length
            features.append(len(domain) if domain else 0)  # domain_length
            features.append(len(path))  # path_length
            features.append(len(query) if query else 0)  # query_length
            features.append(max(0, len(domain.split('.')) - 2) if domain else 0)  # subdomain_count
            features.append(int(any(c.isdigit() for c in domain) if domain else False))  # domain_has_numbers
            features.append(domain.count('-') if domain else 0)  # domain_hyphen_count
            features.append(int(bool(re.search(r'\d+\.\d+\.\d+\.\d+', url))))  # has_ip
            features.append(int('xn--' in domain))  # homograph_attack
            features.append(sum(c.isdigit() for c in url))  # digit_count
            
            # 11-20: Keyword ve i√ßerik analizi
            crypto_keywords = ['wallet', 'crypto', 'bitcoin', 'ethereum', 'btc', 'eth', 'coin']
            features.append(sum(kw in url.lower() for kw in crypto_keywords))  # crypto_keywords_count
            
            phishing_keywords = ['secure', 'account', 'verify', 'login', 'signin', 'confirm']
            features.append(sum(kw in url.lower() for kw in phishing_keywords))  # phishing_keywords_count
            
            brands = ['paypal', 'amazon', 'google', 'microsoft', 'apple', 'facebook']
            features.append(int(any(brand in domain.lower() for brand in brands)))  # brand_impersonation
            
            suspicious_tlds = ['tk', 'ml', 'ga', 'cf', 'xyz', 'top', 'click']
            tld = domain.split('.')[-1] if '.' in domain else ''
            features.append(int(tld in suspicious_tlds))  # tld_suspicious
            
            features.append(sum(c.isupper() for c in url))  # uppercase_count
            features.append(url.count('.'))  # dot_count
            features.append(url.count('/'))  # slash_count
            features.append(url.count('-'))  # hyphen_count
            features.append(url.count('_'))  # underscore_count
            features.append(url.count('%'))  # percent_count
            
            # 21-30: Geli≈ümi≈ü √∂zellikler
            def calculate_entropy(s):
                if not s: return 0
                freq = {}
                for c in s: freq[c] = freq.get(c, 0) + 1
                entropy = 0
                for f in freq.values():
                    p = f / len(s)
                    entropy -= p * math.log2(p)
                return entropy
            
            features.append(calculate_entropy(url))  # url_entropy
            features.append(calculate_entropy(domain) if domain else 0)  # domain_entropy
            features.append(len([x for x in path.split('/') if x]))  # path_depth
            features.append(int(bool(query)))  # has_parameters
            features.append(int(len(domain) < 5 if domain else False))  # short_domain
            features.append(int(len(domain) > 30 if domain else False))  # long_domain
            features.append(len(url) + domain.count('.') + url.count('/'))  # url_complexity
            features.append(int(parsed.scheme == 'https'))  # is_https
            features.append(int(any(ord(c) > 127 for c in url)))  # is_international
            features.append(len(max(re.findall(r'\d+', url), key=len, default="")))  # max_consecutive_digits
            
            return np.array([features])
            
        except Exception as e:
            print(f"Crypto 30-feature extraction error: {e}")
            # Fallback: 30 √∂zellik i√ßin default deƒüerler
            return np.array([[0] * 30])

    def _extract_basic_crypto_features(self, url: str):
        """Cryptocurrency model i√ßin doƒüru √∂zellik √ßƒ±karƒ±mƒ± (46 √∂zellik)"""
        try:
            from urllib.parse import urlparse
            import math
            
            parsed = urlparse(url)
            domain = parsed.netloc
            path = parsed.path
            query = parsed.query
            
            # Crypto model i√ßin doƒüru 46 √∂zellik isimlerine g√∂re
            features = []
            
            # 1. url_length
            features.append(len(url))
            
            # 2. domain_length
            features.append(len(domain) if domain else 0)
            
            # 3. path_length
            features.append(len(path))
            
            # 4. query_length
            features.append(len(query) if query else 0)
            
            # 5. subdomain_count
            subdomains = domain.split('.') if domain else []
            features.append(max(0, len(subdomains) - 2) if len(subdomains) > 1 else 0)
            
            # 6. domain_has_numbers
            features.append(int(any(c.isdigit() for c in domain) if domain else False))
            
            # 7. domain_hyphen_count
            features.append(domain.count('-') if domain else 0)
            
            # 8. crypto_keywords_count
            crypto_keywords = ['wallet', 'crypto', 'bitcoin', 'ethereum', 'btc', 'eth', 'coin', 'blockchain']
            features.append(sum(kw in url.lower() for kw in crypto_keywords))
            
            # 9. phishing_keywords_count
            phishing_keywords = ['secure', 'account', 'verify', 'login', 'signin', 'confirm', 'update']
            features.append(sum(kw in url.lower() for kw in phishing_keywords))
            
            # 10. special_char_count
            special_chars = '@$%&*()+=[]{}|\\:";\'<>?,./'
            features.append(sum(c in special_chars for c in url))
            
            # 11. uppercase_count
            features.append(sum(c.isupper() for c in url))
            
            # 12. digit_count
            features.append(sum(c.isdigit() for c in url))
            
            # 13. has_ip
            features.append(int(bool(re.search(r'\d+\.\d+\.\d+\.\d+', url))))
            
            # 14. has_port
            features.append(int(':' in domain and not domain.endswith(':80') and not domain.endswith(':443')))
            
            # 15. tld_suspicious
            suspicious_tlds = ['tk', 'ml', 'ga', 'cf', 'xyz', 'top', 'click', 'download']
            tld = domain.split('.')[-1] if '.' in domain else ''
            features.append(int(tld in suspicious_tlds))
            
            # 16. url_entropy
            def calculate_entropy(s):
                if not s: return 0
                freq = {}
                for c in s:
                    freq[c] = freq.get(c, 0) + 1
                entropy = 0
                for f in freq.values():
                    p = f / len(s)
                    entropy -= p * math.log2(p)
                return entropy
            features.append(calculate_entropy(url))
            
            # 17. domain_entropy
            features.append(calculate_entropy(domain) if domain else 0)
            
            # 18. has_shortener
            shorteners = ['bit.ly', 'tinyurl', 'short.link', 'is.gd', 't.co']
            features.append(int(any(short in domain for short in shorteners)))
            
            # 19. brand_impersonation
            brands = ['paypal', 'amazon', 'google', 'microsoft', 'apple', 'facebook', 'binance']
            features.append(int(any(brand in domain.lower() for brand in brands)))
            
            # 20. homograph_attack
            features.append(int('xn--' in domain))
            
            # 21. is_international
            features.append(int(any(ord(c) > 127 for c in url)))
            
            # 22. url_complexity
            features.append(len(url) + domain.count('.') + url.count('/') + len(query))
            
            # 23. path_depth
            features.append(len([x for x in path.split('/') if x]))
            
            # 24. has_parameters
            features.append(int(bool(query)))
            
            # 25. suspicious_extensions
            sus_extensions = ['.exe', '.scr', '.bat', '.cmd', '.pif']
            features.append(int(any(ext in path.lower() for ext in sus_extensions)))
            
            # 26. short_domain
            features.append(int(len(domain) < 5 if domain else False))
            
            # 27. long_domain
            features.append(int(len(domain) > 30 if domain else False))
            
            # 28-34. Name features (simulated for URL-only analysis)
            features.extend([
                0,  # name_length (no name field)
                0,  # name_dots
                0,  # name_hyphens
                0,  # name_numbers
                0,  # name_crypto_terms
                0,  # name_multiple_tld
                0   # name_suspicious_tld
            ])
            
            # 35-37. Name analysis (continued)
            features.extend([
                0,  # name_long_subdomain
                0,  # name_brand_similarity
                0   # name_vowel_ratio
            ])
            
            # 38-41. Description features (simulated)
            features.extend([
                0,  # desc_length (no description)
                0,  # desc_scam_words
                0,  # desc_urgency
                0   # desc_trust_words
            ])
            
            # 42-46. Address features (simulated)
            features.extend([
                0,  # addr_count (no addresses)
                0,  # addr_bitcoin_count
                0,  # addr_ethereum_count
                0,  # addr_other_count
                0   # addr_suspicious_count
            ])
            
            return np.array([features])
            
        except Exception as e:
            print(f"Crypto feature extraction error: {e}")
            # Fallback: 46 √∂zellik i√ßin default deƒüerler
            return np.array([[0] * 46])

    def _extract_basic_url_features(self, url: str):
        """Basit URL √∂zellik √ßƒ±karƒ±mƒ± (fallback)"""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            
            features = [
                len(url),  # url_length
                len(parsed.netloc),  # domain_length
                len(parsed.path),  # path_length
                len(parsed.query),  # query_length
                len(parsed.fragment),  # fragment_length
                parsed.netloc.count('.') - 1 if parsed.netloc.count('.') > 0 else 0,  # subdomain_count
                int(any(c.isdigit() for c in parsed.netloc)),  # domain_has_digits
                int('-' in parsed.netloc),  # domain_has_hyphen
                int('_' in parsed.netloc),  # domain_has_underscore
                len(parsed.netloc.split('.')[-1]) if '.' in parsed.netloc else 0,  # tld_length
                int(parsed.netloc.split('.')[-1] in ['com', 'org', 'net', 'edu', 'gov']) if '.' in parsed.netloc else 0,  # is_common_tld
                int(bool(re.search(r'\d+\.\d+\.\d+\.\d+', url))),  # has_ip_address
                int('paypal' in url.lower() or 'bank' in url.lower()),  # has_suspicious_words
                url.count('.'),  # dot_count
                url.count('/'),  # slash_count
                url.count('?'),  # question_count
                url.count('='),  # equal_count
                url.count('&'),  # ampersand_count
                url.count('-'),  # hyphen_count
                url.count('_'),  # underscore_count
                url.count('%'),  # percent_count
                url.count('@'),  # at_count
                int(parsed.scheme == 'https'),  # is_https
                int(parsed.netloc.startswith('www.')),  # has_www
                int('%' in url),  # has_url_encoding
                len(re.findall(r'%[0-9a-fA-F]{2}', url)),  # url_encoding_count
                int(bool(re.search(r'[<>"\'\{\}|\\^`\[\]]', url))),  # has_suspicious_chars
                len([x for x in parsed.path.split('/') if x]),  # path_depth
                int(bool(re.search(r'\.[a-zA-Z]{2,4}$', parsed.path))),  # path_has_extension
                len(parsed.query.split('&')) if parsed.query else 0,  # query_param_count
                2.5,  # domain_entropy (approximate)
                3.0,  # url_entropy (approximate)
                len(re.findall(r'[0-9a-fA-F]{8,}', url)),  # hex_pattern_count
                len(max(re.findall(r'\d+', url), key=len, default="")),  # max_consecutive_digits
                len(max(re.findall(r'[a-zA-Z]+', url), key=len, default=""))  # max_consecutive_letters
            ]
            
            return np.array([features])
        except:
            # En son fallback: 35 √∂zellik i√ßin default deƒüerler
            return np.array([[0] * 35])

    def _extract_basic_link_phishing_features(self, url: str):
        """Link Phishing Detection model i√ßin tam √∂zellik √ßƒ±karƒ±mƒ± (89 √∂zellik)"""
        try:
            from urllib.parse import urlparse
            import math
            
            parsed = urlparse(url)
            domain = parsed.netloc
            path = parsed.path
            query = parsed.query
            
            # Dataset s√ºtun sƒ±rasƒ±na g√∂re 89 √∂zellik (url ve status hari√ß + 5 yeni)
            features = []
            
            # 1. url_length
            features.append(len(url))
            
            # 2. hostname_length  
            features.append(len(domain))
            
            # 3. ip (IP address check)
            features.append(int(bool(re.search(r'\d+\.\d+\.\d+\.\d+', url))))
            
            # 4-19. Special character counts
            features.append(url.count('.'))      # total_of.
            features.append(url.count('-'))      # total_of-
            features.append(url.count('@'))      # total_of@
            features.append(url.count('?'))      # total_of?
            features.append(url.count('&'))      # total_of&
            features.append(url.count('='))      # total_of=
            features.append(url.count('_'))      # total_of_
            features.append(url.count('~'))      # total_of~
            features.append(url.count('%'))      # total_of%
            features.append(url.count('/'))      # total_of/
            features.append(url.count('*'))      # total_of*
            features.append(url.count(':'))      # total_of:
            features.append(url.count(','))      # total_of,
            features.append(url.count(';'))      # total_of;
            features.append(url.count('$'))      # total_of$
            
            # 20-22. Common patterns
            features.append(url.lower().count('www'))     # total_of_www
            features.append(url.lower().count('com'))     # total_of_com
            features.append(path.lower().count('http'))   # total_of_http_in_path
            
            # 23. https_token
            features.append(int(parsed.scheme == 'https'))
            
            # 24-25. Digit ratios
            features.append(sum(c.isdigit() for c in url) / len(url) if url else 0)        # ratio_digits_url
            features.append(sum(c.isdigit() for c in domain) / len(domain) if domain else 0)  # ratio_digits_host
            
            # 26. punycode
            features.append(int('xn--' in domain))
            
            # 27. port
            features.append(int(':' in domain and not domain.endswith(':80') and not domain.endswith(':443')))
            
            # 28-29. TLD analysis
            tld_in_path = int(any(tld in path for tld in ['.com', '.org', '.net', '.edu']))
            tld_in_subdomain = int(any(tld in domain for tld in ['.com', '.org', '.net', '.edu']))
            features.append(tld_in_path)
            features.append(tld_in_subdomain)
            
            # 30-33. Subdomain analysis
            subdomains = domain.split('.')
            nb_subdomains = max(0, len(subdomains) - 2) if len(subdomains) > 1 else 0
            features.append(int(nb_subdomains > 3))  # abnormal_subdomain
            features.append(nb_subdomains)           # nb_subdomains
            features.append(int('-' in domain))     # prefix_suffix
            features.append(0)                      # random_domain (simplified)
            
            # 34-38. Service and extension analysis
            shortening_services = ['bit.ly', 'tinyurl', 'short.link', 'is.gd', 't.co']
            features.append(int(any(service in domain for service in shortening_services)))  # shortening_service
            features.append(int(bool(re.search(r'\.[a-zA-Z]{2,4}$', path))))                # path_extension
            features.append(0)  # nb_redirection (simplified)
            features.append(0)  # nb_external_redirection (simplified)
            
            # 39-49. Word analysis
            words_raw = re.findall(r'[a-zA-Z]+', url)
            length_words_raw = len(words_raw)
            
            if words_raw:
                shortest_words_raw = min(len(w) for w in words_raw)
                longest_words_raw = max(len(w) for w in words_raw)
                avg_words_raw = sum(len(w) for w in words_raw) / len(words_raw)
            else:
                shortest_words_raw = 0
                longest_words_raw = 0
                avg_words_raw = 0
            
            host_words = re.findall(r'[a-zA-Z]+', domain)
            if host_words:
                shortest_word_host = min(len(w) for w in host_words)
                longest_word_host = max(len(w) for w in host_words)
                avg_word_host = sum(len(w) for w in host_words) / len(host_words)
            else:
                shortest_word_host = 0
                longest_word_host = 0
                avg_word_host = 0
            
            path_words = re.findall(r'[a-zA-Z]+', path)
            if path_words:
                shortest_word_path = min(len(w) for w in path_words)
                longest_word_path = max(len(w) for w in path_words)
                avg_word_path = sum(len(w) for w in path_words) / len(path_words)
            else:
                shortest_word_path = 0
                longest_word_path = 0
                avg_word_path = 0
            
            features.extend([
                length_words_raw, int(0),  # char_repeat (simplified)
                shortest_words_raw, shortest_word_host, shortest_word_path,
                longest_words_raw, longest_word_host, longest_word_path,
                avg_words_raw, avg_word_host, avg_word_path
            ])
            
            # 50-55. Phishing and brand analysis
            phish_keywords = ['secure', 'account', 'webscr', 'login', 'ebayisapi', 'signin', 'banking', 'confirm']
            brands = ['paypal', 'amazon', 'google', 'microsoft', 'apple', 'facebook', 'ebay', 'bank']
            suspicious_tlds = ['tk', 'ml', 'ga', 'cf', 'xyz', 'top', 'click']
            
            tld = domain.split('.')[-1] if '.' in domain else ''
            
            features.extend([
                sum(keyword in url.lower() for keyword in phish_keywords),  # phish_hints
                int(any(brand in domain.lower() for brand in brands)),      # domain_in_brand
                int(any(brand in '.'.join(subdomains[:-2]) for brand in brands) if len(subdomains) > 2 else False),  # brand_in_subdomain
                int(any(brand in path.lower() for brand in brands)),        # brand_in_path
                int(tld in suspicious_tlds),                                # suspecious_tld
                int(any(word in url.lower() for word in ['malware', 'spam', 'phish']))  # statistical_report
            ])
            
            # 56-75. Web content analysis (simplified for URL-only analysis)
            features.extend([
                len(re.findall(r'http[s]?://', url)) - 1,  # nb_hyperlinks
                0.8,  # ratio_intHyperlinks (default)
                0.2,  # ratio_extHyperlinks (default) 
                0,    # ratio_nullHyperlinks
                0,    # nb_extCSS
                0,    # ratio_intRedirection
                0.0,  # ratio_extRedirection
                0,    # ratio_intErrors
                0.0,  # ratio_extErrors
                int(any(word in url.lower() for word in ['login', 'signin', 'auth'])),  # login_form
                0,    # external_favicon
                0.0,  # links_in_tags
                0,    # submit_email
                0.0,  # ratio_intMedia
                0.0,  # ratio_extMedia
                0,    # sfh
                0,    # iframe
                0,    # popup_window
                0.8,  # safe_anchor (default)
                0,    # onmouseover
                0     # right_clic
            ])
            
            # 76-84. Domain and reputation analysis (simplified)
            features.extend([
                0,  # empty_title
                int(any(brand in domain.lower() for brand in brands)),  # domain_in_title (approximation)
                1,  # domain_with_copyright (assume has copyright: "one")
                1,  # whois_registered_domain (assume registered)
                365,  # domain_registration_length (default 1 year)
                1000,  # domain_age (default old domain)
                5,    # web_traffic (medium traffic)
                1,    # dns_record (assume exists)
                1,    # google_index (assume indexed)
                3     # page_rank (medium rank)
            ])
            
            # 85-89. Composite features (yeni eklenen 5 √∂zellik)
            features.extend([
                features[0] + features[1] + nb_subdomains,  # url_complexity_score
                features[22] + (1 if len(features) > 83 else 0) + (1 if len(features) > 84 else 0),  # security_score
                features[2] + features[26] + (1 if len(features) > 30 else 0) + (1 if len(features) > 32 else 0) + (1 if len(features) > 34 else 0),  # suspicious_pattern_score
                (features[51] if len(features) > 51 else 0) + (features[52] if len(features) > 52 else 0) + (features[53] if len(features) > 53 else 0),  # brand_mimicking_score
                sum(features[i] if len(features) > i else 0 for i in [69, 70, 76, 77, 78, 79])  # malicious_content_score
            ])
            
            return np.array([features])
            
        except Exception as e:
            print(f"Link phishing feature extraction error: {e}")
            # Fallback: 89 √∂zellik i√ßin default deƒüerler
            return np.array([[0] * 89])
    
    def _extract_basic_malicious_urls_features(self, url: str):
        """Malicious URLs i√ßin temel √∂zellik √ßƒ±karƒ±mƒ±"""
        try:
            from urllib.parse import urlparse
            from collections import Counter
            import math
            
            parsed = urlparse(url)
            domain = parsed.netloc
            path = parsed.path
            query = parsed.query
            
            features = []
            
            # Temel URL √∂zellikleri
            features.extend([
                len(url),                                    # url_length
                len(domain),                                 # domain_length  
                len(path),                                   # path_length
                len(query) if query else 0,                  # query_length
                len(parsed.fragment) if parsed.fragment else 0,  # fragment_length
            ])
            
            # Domain analizi
            if domain:
                domain_parts = domain.split('.')
                features.extend([
                    max(0, len(domain_parts) - 2),          # subdomain_count
                    len(domain_parts),                       # domain_parts_count
                    len(domain_parts[-1]) if domain_parts else 0,  # tld_length
                    int(domain_parts[-1] in ['tk', 'ml', 'ga', 'cf', 'xyz', 'top', 'click'] if domain_parts else False),  # has_suspicious_tld
                    sum(c.isdigit() for c in domain) / len(domain) if domain else 0,  # domain_digit_ratio
                    domain.count('-'),                       # domain_hyphen_count
                    domain.count('_'),                       # domain_underscore_count
                ])
            else:
                features.extend([0, 0, 0, 0, 0, 0, 0])
            
            # Path ve query analizi
            features.extend([
                len([x for x in path.split('/') if x]),     # path_depth
                int('.' in path.split('/')[-1] if path.split('/') else False),  # path_has_extension
                len(query.split('&')) if query else 0,      # query_params_count
            ])
            
            # Protokol ve √∂zel karakterler
            features.extend([
                int(parsed.scheme == 'https'),              # is_https
                int(parsed.port is not None),               # has_port
                parsed.port if parsed.port else 0,         # port_number
                sum(c in '@$%&*()+=[]{}|\\:";\'<>?,./' for c in url),  # special_char_count
                sum(c.isdigit() for c in url),              # total_digits
            ])
            
            # Pattern analizi
            suspicious_patterns = [
                r'[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}',  # IP adresi
                r'localhost',
                r'%[0-9a-fA-F]{2}',  # URL encoding
            ]
            
            for pattern in suspicious_patterns:
                features.append(int(bool(re.search(pattern, url))))
            
            # Keyword analizi
            phishing_keywords = ['secure', 'account', 'verify', 'login', 'signin', 'confirm', 'update']
            defacement_keywords = ['hacked', 'defaced', 'owned', 'pwned']
            malware_keywords = ['download', 'exe', 'payload', 'exploit']
            brands = ['google', 'microsoft', 'apple', 'amazon', 'paypal']
            
            features.extend([
                sum(keyword in url.lower() for keyword in phishing_keywords),
                sum(keyword in url.lower() for keyword in defacement_keywords), 
                sum(keyword in url.lower() for keyword in malware_keywords),
                sum(brand in url.lower() for brand in brands),
            ])
            
            # Entropy hesaplama
            def calculate_entropy(text):
                if not text:
                    return 0
                frequencies = Counter(text)
                length = len(text)
                entropy = 0
                for freq in frequencies.values():
                    prob = freq / length
                    entropy -= prob * math.log2(prob)
                return entropy
            
            features.extend([
                calculate_entropy(url),                     # url_entropy
                calculate_entropy(domain) if domain else 0, # domain_entropy
                calculate_entropy(path),                    # path_entropy
                calculate_entropy(query) if query else 0,  # query_entropy
            ])
            
            # Homograph ve shortener analizi
            features.extend([
                int('xn--' in url.lower()),                 # has_punycode
                int(any(ord(c) > 127 for c in url)),        # mixed_charset
                int(any(shortener in domain.lower() for shortener in ['bit.ly', 'tinyurl', 't.co']) if domain else False),  # is_shortened
            ])
            
            # Tam olarak 43 √∂zelliƒüe √ßƒ±kar (SelectKBest 43 bekliyor)
            while len(features) < 43:
                features.append(0)
            
            # 43'ten fazla varsa kes
            features = features[:43]
            
            return np.array([features])
            
        except Exception as e:
            print(f"Malicious URLs feature extraction error: {e}")
            return np.array([[0] * 43])
    
    def _calculate_reputation(self, url: str) -> float:
        """URL reputation hesapla"""
        score = 0.0
        
        # Basic heuristics
        if 'https' in url:
            score += 10
        if len(url) < 50:
            score += 5
        if url.count('.') <= 2:
            score += 5
        
        # Suspicious patterns
        suspicious_patterns = ['bit.ly', 'tinyurl', 'secure', 'verify', 'account', 'login']
        for pattern in suspicious_patterns:
            if pattern in url.lower():
                score -= 20
        
        return max(-100, min(100, score))
    
    def predict_single_model(self, url: str, model_name: str) -> Optional[Dict]:
        """Tek model ile tahmin yap"""
        
        if model_name not in self.models:
            return None
        
        try:
            # √ñzellik √ßƒ±karƒ±mƒ±
            features_array = self.extract_features_for_model(url, model_name)
            
            if features_array is None:
                return None
            
            # Model tahmini
            model = self.models[model_name]
            
            # Prediction
            prediction = model.predict(features_array)[0]
            prediction_proba = model.predict_proba(features_array)[0]
            
            # Confidence score
            confidence = max(prediction_proba)
            
            # Model-specific label handling
            if model_name == 'phishing_urls_model':
                # Bu model: 0=bad(phishing), 1=good(safe)
                result = {
                    'prediction': 1 - int(prediction),  # Reverse: 0->1(phishing), 1->0(safe)
                    'prediction_label': 'Safe' if prediction == 1 else 'Phishing',
                    'confidence': float(confidence),
                    'probability_safe': float(prediction_proba[1]),  # good probability
                    'probability_phishing': float(prediction_proba[0])  # bad probability
                }
            elif model_name == 'crypto_scam_model':
                # Crypto model: multi-class (Phishing, Scamming, Malware)
                # Phishing/Scamming/Malware -> 1 (threat), otherwise -> 0 (safe)
                label_encoder = self.model_info[model_name]['label_encoder']
                predicted_category = label_encoder.inverse_transform([prediction])[0]
                
                # Convert to binary: threat categories -> 1, others -> 0
                is_threat = predicted_category in ['Phishing', 'Scamming', 'Malware']
                
                result = {
                    'prediction': int(is_threat),
                    'prediction_label': 'Phishing' if is_threat else 'Safe',
                    'confidence': float(confidence),
                    'probability_safe': float(1 - confidence if is_threat else confidence),
                    'probability_phishing': float(confidence if is_threat else 1 - confidence),
                    'crypto_category': predicted_category  # Extra bilgi
                }
            elif model_name == 'link_phishing_model':
                # Link phishing model: 0=legitimate, 1=phishing
                label_encoder = self.model_info[model_name]['label_encoder']
                predicted_category = label_encoder.inverse_transform([prediction])[0]
                
                is_phishing = predicted_category == 'phishing'
                
                result = {
                    'prediction': int(is_phishing),
                    'prediction_label': 'Phishing' if is_phishing else 'Safe',
                    'confidence': float(confidence),
                    'probability_safe': float(prediction_proba[0] if not is_phishing else prediction_proba[1]),
                    'probability_phishing': float(prediction_proba[1] if is_phishing else prediction_proba[0]),
                    'link_category': predicted_category  # Extra bilgi
                }
            elif model_name == 'malicious_urls_model':
                # Malicious URLs model: 0=benign, 1=threat (phishing/malware/defacement)
                is_threat = prediction == 1
                
                result = {
                    'prediction': int(prediction),
                    'prediction_label': 'Phishing' if is_threat else 'Safe',
                    'confidence': float(confidence),
                    'probability_safe': float(prediction_proba[0]),
                    'probability_phishing': float(prediction_proba[1]),
                    'threat_type': 'multi-threat'  # phishing, malware, or defacement
                }
            else:
                # Diƒüer modeller: 0=safe, 1=phishing
                result = {
                    'prediction': int(prediction),
                    'prediction_label': 'Phishing' if prediction == 1 else 'Safe',
                    'confidence': float(confidence),
                    'probability_safe': float(prediction_proba[0]),
                    'probability_phishing': float(prediction_proba[1]) if len(prediction_proba) > 1 else 1.0 - float(prediction_proba[0])
                }
            
            return result
            
        except Exception as e:
            print(f"‚ùå {model_name} tahmin hatasƒ±: {e}")
            return None
    
    def predict_ensemble(self, url: str) -> Dict:
        """Ensemble tahmin - t√ºm modelleri kullan"""
        
        print(f"üîç Ensemble analizi ba≈ülƒ±yor: {url}")
        
        # Individual model predictions
        model_predictions = {}
        valid_predictions = []
        
        # Sadece ger√ßek ML modellerini test et (auxiliary objects deƒüil)
        actual_models = [
            'phishing_model', 'cybersecurity_model', 'phishing_urls_model', 
            'website_model', 'crypto_scam_model', 'link_phishing_model', 'malicious_urls_model'
        ]
        
        for model_name in actual_models:
            if model_name in self.models:
                result = self.predict_single_model(url, model_name)
                if result:
                    model_predictions[model_name] = result
                    valid_predictions.append(result)
                    
                    print(f"   üìä {model_name}: {result['prediction_label']} ({result['confidence']:.3f})")
        
        if not valid_predictions:
            return {
                'error': 'No valid predictions from models',
                'url': url,
                'timestamp': datetime.now().isoformat()
            }
        
        # Weighted voting
        weighted_safe_score = 0.0
        weighted_phishing_score = 0.0
        total_weight = 0.0
        
        for model_name, prediction in model_predictions.items():
            weight = self.model_weights.get(model_name, 1.0)
            
            weighted_safe_score += prediction['probability_safe'] * weight
            weighted_phishing_score += prediction['probability_phishing'] * weight
            total_weight += weight
        
        # Normalize
        if total_weight > 0:
            final_safe_prob = weighted_safe_score / total_weight
            final_phishing_prob = weighted_phishing_score / total_weight
        else:
            final_safe_prob = 0.5
            final_phishing_prob = 0.5
        
        # Final decision
        final_prediction = 1 if final_phishing_prob > final_safe_prob else 0
        final_confidence = max(final_safe_prob, final_phishing_prob)
        
        # Voting statistics
        safe_votes = sum(1 for pred in valid_predictions if pred['prediction'] == 0)
        phishing_votes = sum(1 for pred in valid_predictions if pred['prediction'] == 1)
        
        # Rule-based analysis
        try:
            basic_features = self.feature_extractor.extract_features(url)
            rule_flags = self.rule_analyzer.analyze(url, basic_features)
            rule_result = {'flags': rule_flags, 'risk_score': len(rule_flags) * 20}
        except Exception as e:
            print(f"‚ùå Rule-based analysis hatasƒ±: {e}")
            rule_result = {'flags': [], 'risk_score': 0}
        
        # Final ensemble result
        ensemble_result = {
            'url': url,
            'final_prediction': final_prediction,
            'final_label': 'Phishing' if final_prediction == 1 else 'Safe',
            'confidence': float(final_confidence),
            'probability_safe': float(final_safe_prob),
            'probability_phishing': float(final_phishing_prob),
            
            # Voting details
            'total_models': 7,  # Sabit 7 model
            'active_models': len(valid_predictions),
            'safe_votes': safe_votes,
            'phishing_votes': phishing_votes,
            'voting_ratio': f"{phishing_votes}/{safe_votes + phishing_votes}",
            
            # Individual model results
            'model_predictions': model_predictions,
            
            # Rule-based analysis
            'rule_analysis': rule_result,
            
            # Metadata
            'timestamp': datetime.now().isoformat(),
            'model_weights': self.model_weights
        }
        
        print(f"   üéØ Ensemble Sonu√ß: {ensemble_result['final_label']} ({ensemble_result['confidence']:.3f})")
        print(f"   üìä Voting: {phishing_votes} Phishing, {safe_votes} Safe")
        
        return ensemble_result
    
    def update_model_weights(self, feedback: Dict):
        """Kullanƒ±cƒ± geri bildirimine g√∂re model aƒüƒ±rlƒ±klarƒ±nƒ± g√ºncelle"""
        
        if 'model_predictions' not in feedback or 'user_label' not in feedback:
            return
        
        user_label = feedback['user_label']  # 0: Safe, 1: Phishing
        model_predictions = feedback['model_predictions']
        
        # Her model i√ßin doƒüruluk hesapla
        for model_name, prediction in model_predictions.items():
            if model_name in self.model_weights:
                predicted_label = prediction['prediction']
                
                # Doƒüru tahmin ise aƒüƒ±rlƒ±ƒüƒ± artƒ±r, yanlƒ±≈ü ise azalt
                if predicted_label == user_label:
                    self.model_weights[model_name] *= 1.05  # %5 artƒ±r
                else:
                    self.model_weights[model_name] *= 0.95  # %5 azalt
                
                # Minimum aƒüƒ±rlƒ±k sƒ±nƒ±rƒ±
                self.model_weights[model_name] = max(0.1, self.model_weights[model_name])
        
        # Aƒüƒ±rlƒ±klarƒ± normalize et
        total_weight = sum(self.model_weights.values())
        if total_weight > 0:
            for model_name in self.model_weights:
                self.model_weights[model_name] /= total_weight
        
        print(f"üîÑ Model aƒüƒ±rlƒ±klarƒ± g√ºncellendi:")
        for model_name, weight in self.model_weights.items():
            print(f"   {model_name}: {weight:.3f}")
    
    def save_feedback(self, feedback: Dict):
        """Geri bildirimi kaydet"""
        
        feedback['feedback_timestamp'] = datetime.now().isoformat()
        self.feedback_history.append(feedback)
        
        # CSV'ye kaydet
        feedback_df = pd.DataFrame([feedback])
        
        try:
            existing_feedback = pd.read_csv('ensemble_feedback.csv')
            updated_feedback = pd.concat([existing_feedback, feedback_df], ignore_index=True)
        except FileNotFoundError:
            updated_feedback = feedback_df
        
        updated_feedback.to_csv('ensemble_feedback.csv', index=False)
        
        # Model aƒüƒ±rlƒ±klarƒ±nƒ± g√ºncelle
        self.update_model_weights(feedback)
        
        print(f"üíæ Geri bildirim kaydedildi ve model aƒüƒ±rlƒ±klarƒ± g√ºncellendi")
    
    def get_model_info(self) -> Dict:
        """Model bilgilerini d√∂nd√ºr"""
        
        return {
            'ensemble_info': {
                'total_models': len(self.models),
                'active_models': len([m for m in self.models.values() if m is not None]),
                'model_weights': self.model_weights,
                'feedback_count': len(self.feedback_history)
            },
            'individual_models': self.model_info,
            'performance_metrics': self.model_performance
        }
    
    def analyze_url_comprehensive(self, url: str) -> Dict:
        """Kapsamlƒ± URL analizi"""
        
        # Ensemble prediction
        ensemble_result = self.predict_ensemble(url)
        
        # Add comprehensive analysis
        comprehensive_result = {
            **ensemble_result,
            'analysis_details': {
                'feature_analysis': self.feature_extractor.extract_features(url),
                'model_comparison': self._compare_model_strengths(ensemble_result),
                'risk_factors': self._identify_risk_factors(url, ensemble_result),
                'recommendations': self._generate_recommendations(ensemble_result)
            }
        }
        
        return comprehensive_result
    
    def _compare_model_strengths(self, ensemble_result: Dict) -> Dict:
        """Model g√º√ßl√º yanlarƒ±nƒ± kar≈üƒ±la≈ütƒ±r"""
        
        model_strengths = {}
        
        for model_name, info in self.model_info.items():
            if model_name in ensemble_result.get('model_predictions', {}):
                prediction = ensemble_result['model_predictions'][model_name]
                
                model_strengths[model_name] = {
                    'speciality': info.get('speciality', info.get('specialization', 'URL analysis')),
                    'confidence': prediction['confidence'],
                    'decision': prediction['prediction_label'],
                    'algorithm': info.get('algorithm', 'Machine Learning')
                }
        
        return model_strengths
    
    def _identify_risk_factors(self, url: str, ensemble_result: Dict) -> List[str]:
        """Risk fakt√∂rlerini belirle"""
        
        risk_factors = []
        
        # Rule-based risk factors
        if ensemble_result.get('rule_analysis', {}).get('risk_score', 0) > 50:
            risk_factors.append("High rule-based risk score")
        
        # Model consensus
        if ensemble_result.get('phishing_votes', 0) > ensemble_result.get('safe_votes', 0):
            risk_factors.append("Majority of models predict phishing")
        
        # Low confidence
        if ensemble_result.get('confidence', 1.0) < 0.7:
            risk_factors.append("Low prediction confidence")
        
        # URL characteristics
        if len(url) > 100:
            risk_factors.append("Unusually long URL")
        
        if url.count('.') > 5:
            risk_factors.append("Too many subdomains")
        
        return risk_factors
    
    def _generate_recommendations(self, ensemble_result: Dict) -> List[str]:
        """√ñneriler olu≈ütur"""
        
        recommendations = []
        
        if ensemble_result['final_prediction'] == 1:  # Phishing
            recommendations.append("üö® Bu URL'ye tƒ±klamayƒ±n")
            recommendations.append("üîç URL'yi dikkatli inceleyin")
            recommendations.append("üõ°Ô∏è G√ºvenilir kaynaklardan doƒürulayƒ±n")
        else:  # Safe
            if ensemble_result['confidence'] < 0.8:
                recommendations.append("‚ö†Ô∏è Orta d√ºzeyde g√ºvenlik riski")
                recommendations.append("üîç Ek doƒürulama yapƒ±labilir")
            else:
                recommendations.append("‚úÖ URL g√ºvenli g√∂r√ºn√ºyor")
        
        return recommendations

    def extract_all_features_once(self, url: str):
        """T√ºm √∂zellikleri tek seferde √ßƒ±kar - OPTIMIZE EDƒ∞LMƒ∞≈û"""
        print("üöÄ OPTIMIZE EDƒ∞LMƒ∞≈û: T√ºm √∂zellikler tek seferde √ßƒ±karƒ±lƒ±yor...")
        
        features_cache = {}
        
        try:
            # 1. Temel URL √∂zellikleri (t√ºm modeller i√ßin gerekli)
            basic_features = self.feature_extractor.extract_features(url)
            features_cache['basic'] = basic_features
            
            # 2. Cybersecurity model √∂zellikleri (simulated)
            cybersecurity_features = [
                int('.onion' in url.lower()),  # is_onion
                self._extract_tld(url),  # tld
                0,  # categories_sophos
                0,  # categories_alpha_mountain
                self._calculate_reputation(url),  # reputation
                0,  # number_of_tags
                60,  # last_analysis_stats_harmles
                5,  # last_analysis_stats_malicious
                0,  # last_analysis_stats_suspicious
                25,  # last_analysis_stats_undetected
                0,  # total_votes_harmless
                0,  # total_votes_malicious
                5/61,  # malicious_harmless_ratio
                90,  # total_analysis_score
                2  # reputation_category
            ]
            features_cache['cybersecurity'] = cybersecurity_features
            
            # 3. Website features (eƒüer extractor varsa)
            if 'website_extractor' in self.models:
                try:
                    extractor = self.models['website_extractor']
                    website_features = extractor.extract_website_features(url)
                    if 'class' in website_features:
                        del website_features['class']
                    
                    import pandas as pd
                    features_df = pd.DataFrame([website_features])
                    features_engineered = extractor.feature_engineering(features_df)
                    features_cache['website'] = features_engineered
                except Exception as e:
                    print(f"‚ö†Ô∏è Website features fallback: {e}")
                    features_cache['website'] = self._extract_basic_website_features(url)
            else:
                features_cache['website'] = self._extract_basic_website_features(url)
            
            # 4. Crypto features (pipeline ile)
            if 'crypto_scam_model' in self.model_info and 'pipeline' in self.model_info['crypto_scam_model']:
                try:
                    pipeline = self.model_info['crypto_scam_model']['pipeline']
                    import pandas as pd
                    temp_df = pd.DataFrame({
                        'url': [url],
                        'name': [''],
                        'description': [''],
                        'addresses': ['']
                    })
                    crypto_features = pipeline.create_features(temp_df)
                    features_cache['crypto'] = crypto_features
                except Exception as e:
                    print(f"‚ö†Ô∏è Crypto features fallback: {e}")
                    features_cache['crypto'] = self._extract_basic_crypto_features_30(url)
            else:
                features_cache['crypto'] = self._extract_basic_crypto_features_30(url)
            
            # 5. URL-specific features
            features_cache['url_specific'] = self._extract_basic_url_features(url)
            
            # 6. Link phishing features
            features_cache['link_phishing'] = self._extract_basic_link_phishing_features(url)
            
            # 7. Malicious URLs features
            features_cache['malicious_urls'] = self._extract_basic_malicious_urls_features(url)
            
            print("‚úÖ T√ºm √∂zellikler ba≈üarƒ±yla √ßƒ±karƒ±ldƒ±!")
            return features_cache
            
        except Exception as e:
            print(f"‚ùå √ñzellik √ßƒ±karƒ±m hatasƒ±: {e}")
            return {}
    
    def get_model_features_from_cache(self, url: str, model_name: str, features_cache: Dict):
        """Cache'den belirli model i√ßin √∂zellikleri al - OPTIMIZE EDƒ∞LMƒ∞≈û"""
        
        try:
            if model_name == 'phishing_model':
                # Mega phishing dataset √∂zellikleri
                basic_features = features_cache.get('basic', {})
                
                if 'selected_features' in self.model_info[model_name]:
                    selected_features = self.model_info[model_name]['selected_features']
                    feature_values = []
                    for feature_name in selected_features:
                        feature_values.append(basic_features.get(feature_name, 0))
                    return np.array([feature_values])
                else:
                    feature_values = list(basic_features.values())
                    return np.array([feature_values])
            
            elif model_name == 'cybersecurity_model':
                cybersecurity_features = features_cache.get('cybersecurity', [])
                return np.array([cybersecurity_features])
            
            elif model_name == 'phishing_urls_model':
                # URL √∂zellikleri kullan
                url_features = features_cache.get('url_specific')
                if url_features is not None:
                    # Feature selection uygula
                    if 'phishing_urls_model_feature_selector' in self.models:
                        feature_selector = self.models['phishing_urls_model_feature_selector']
                        return feature_selector.transform(url_features)
                    return url_features
                else:
                    return self._extract_basic_url_features(url)
            
            elif model_name == 'website_model':
                website_features = features_cache.get('website')
                if website_features is not None:
                    # Selected features'a g√∂re sƒ±rala
                    if 'selected_features' in self.model_info[model_name]:
                        selected_features = self.model_info[model_name]['selected_features']
                        available_features = [col for col in selected_features if col in website_features.columns]
                        if available_features:
                            return website_features[available_features].values
                    
                    # Feature selection uygula
                    if 'feature_selector' in self.model_info[model_name]:
                        feature_selector = self.model_info[model_name]['feature_selector']
                        return feature_selector.transform(website_features)
                    
                    return website_features.values
                else:
                    return self._extract_basic_website_features(url)
            
            elif model_name == 'crypto_scam_model':
                crypto_features = features_cache.get('crypto')
                if crypto_features is not None:
                    # Scaling uygula
                    if 'scaler' in self.model_info[model_name]:
                        scaler = self.model_info[model_name]['scaler']
                        crypto_features = scaler.transform(crypto_features)
                    
                    # Feature selection uygula
                    if 'feature_selector' in self.model_info[model_name]:
                        feature_selector = self.model_info[model_name]['feature_selector']
                        crypto_features = feature_selector.transform(crypto_features)
                    
                    return crypto_features
                else:
                    return np.array([[0] * 30])
            
            elif model_name == 'link_phishing_model':
                link_features = features_cache.get('link_phishing')
                if link_features is not None:
                    # Feature selection uygula
                    if 'feature_selector' in self.model_info[model_name]:
                        feature_selector = self.model_info[model_name]['feature_selector']
                        return feature_selector.transform(link_features)
                    return link_features
                else:
                    return self._extract_basic_link_phishing_features(url)
            
            elif model_name == 'malicious_urls_model':
                malicious_features = features_cache.get('malicious_urls')
                if malicious_features is not None:
                    # Feature selection uygula
                    if 'feature_selector' in self.model_info[model_name]:
                        feature_selector = self.model_info[model_name]['feature_selector']
                        return feature_selector.transform(malicious_features)
                    return malicious_features
                else:
                    return self._extract_basic_malicious_urls_features(url)
            
            else:
                print(f"‚ö†Ô∏è Bilinmeyen model: {model_name}")
                return None
                
        except Exception as e:
            print(f"‚ùå {model_name} √∂zellik hatasƒ±: {e}")
            return None

    def predict_ensemble_optimized(self, url: str) -> Dict:
        """OPTIMIZE EDƒ∞LMƒ∞≈û ensemble tahmin - t√ºm √∂zellikleri tek seferde √ßƒ±kar"""
        
        print(f"üöÄ OPTIMIZE EDƒ∞LMƒ∞≈û ensemble analizi ba≈ülƒ±yor: {url}")
        
        # T√ºm √∂zellikleri tek seferde √ßƒ±kar
        features_cache = self.extract_all_features_once(url)
        
        if not features_cache:
            return {
                'error': 'Feature extraction failed',
                'url': url,
                'timestamp': datetime.now().isoformat()
            }
        
        # Individual model predictions
        model_predictions = {}
        valid_predictions = []
        
        # Sadece ger√ßek ML modellerini test et
        actual_models = [
            'phishing_model', 'cybersecurity_model', 'phishing_urls_model', 
            'website_model', 'link_phishing_model', 'malicious_urls_model'
        ]
        # crypto_scam_model ge√ßici olarak devre dƒ±≈üƒ± (√∂zellik uyumsuzluƒüu)
        
        for model_name in actual_models:
            if model_name in self.models and self.models[model_name] is not None:
                try:
                    # Cache'den √∂zellik al (optimize edilmi≈ü)
                    features = self.get_model_features_from_cache(url, model_name, features_cache)
                    
                    if features is not None:
                        # Model prediction
                        model = self.models[model_name]
                        probabilities = model.predict_proba(features)[0]
                        prediction = model.predict(features)[0]
                        
                        # Model explanation olu≈ütur
                        explanation = self._generate_model_explanation(url, model_name, prediction, probabilities, features)
                        
                        prediction_result = {
                            'prediction': int(prediction),
                            'prediction_label': 'Phishing' if prediction == 1 else 'Safe',
                            'probability_safe': float(probabilities[0]),
                            'probability_phishing': float(probabilities[1]) if len(probabilities) > 1 else float(1 - probabilities[0]),
                            'confidence': float(max(probabilities)),
                            'model_name': model_name,
                            'explanation': explanation
                        }
                        
                        model_predictions[model_name] = prediction_result
                        valid_predictions.append(prediction_result)
                        
                        print(f"   üìä {model_name}: {prediction_result['prediction_label']} ({prediction_result['confidence']:.3f})")
                    
                except Exception as e:
                    print(f"   ‚ùå {model_name} hatasƒ±: {e}")
                    continue
        
        if not valid_predictions:
            return {
                'error': 'No valid predictions from models',
                'url': url,
                'timestamp': datetime.now().isoformat()
            }
        
        # Weighted voting
        weighted_safe_score = 0.0
        weighted_phishing_score = 0.0
        total_weight = 0.0
        
        for model_name, prediction in model_predictions.items():
            weight = self.model_weights.get(model_name, 1.0)
            
            weighted_safe_score += prediction['probability_safe'] * weight
            weighted_phishing_score += prediction['probability_phishing'] * weight
            total_weight += weight
        
        # Normalize
        if total_weight > 0:
            final_safe_prob = weighted_safe_score / total_weight
            final_phishing_prob = weighted_phishing_score / total_weight
        else:
            final_safe_prob = 0.5
            final_phishing_prob = 0.5
        
        # Final decision
        final_prediction = 1 if final_phishing_prob > final_safe_prob else 0
        final_confidence = max(final_safe_prob, final_phishing_prob)
        
        # Voting statistics
        safe_votes = sum(1 for pred in valid_predictions if pred['prediction'] == 0)
        phishing_votes = sum(1 for pred in valid_predictions if pred['prediction'] == 1)
        
        # Rule-based analysis (basic features zaten cache'de)
        try:
            basic_features = features_cache.get('basic', {})
            rule_flags = self.rule_analyzer.analyze(url, basic_features)
            rule_result = {'flags': rule_flags, 'risk_score': len(rule_flags) * 20}
        except Exception as e:
            print(f"‚ùå Rule-based analysis hatasƒ±: {e}")
            rule_result = {'flags': [], 'risk_score': 0}
        
        # Final ensemble result
        ensemble_result = {
            'url': url,
            'final_prediction': final_prediction,
            'final_label': 'Phishing' if final_prediction == 1 else 'Safe',
            'confidence': float(final_confidence),
            'probability_safe': float(final_safe_prob),
            'probability_phishing': float(final_phishing_prob),
            
            # Voting details
            'total_models': 7,
            'active_models': len(valid_predictions),
            'safe_votes': safe_votes,
            'phishing_votes': phishing_votes,
            'voting_ratio': f"{phishing_votes}/{safe_votes + phishing_votes}",
            
            # Individual model results
            'model_predictions': model_predictions,
            
            # Rule-based analysis
            'rule_analysis': rule_result,
            
            # Metadata
            'timestamp': datetime.now().isoformat(),
            'model_weights': self.model_weights,
            'optimization': 'features_cached'
        }
        
        print(f"   üéØ OPTIMIZE EDƒ∞LMƒ∞≈û Sonu√ß: {ensemble_result['final_label']} ({ensemble_result['confidence']:.3f})")
        print(f"   üìä Voting: {phishing_votes} Phishing, {safe_votes} Safe")
        
        return ensemble_result

    def _generate_model_explanation(self, url: str, model_name: str, prediction: int, probabilities: np.ndarray, features: np.ndarray) -> Dict:
        """Model kararƒ± i√ßin a√ßƒ±klama olu≈ütur"""
        
        try:
            explanation = {
                'decision_reason': '',
                'key_features': [],
                'risk_factors': [],
                'confidence_factors': []
            }
            
            is_phishing = prediction == 1
            confidence = float(max(probabilities))
            
            # Domain bilgisi √ßƒ±kar
            domain = url.split('/')[2] if '//' in url else url.split('/')[0]
            
            # Model √∂zelliklerine g√∂re a√ßƒ±klama
            if model_name == 'phishing_model':
                # Mega Phishing Model a√ßƒ±klamasƒ±
                if is_phishing:
                    explanation['decision_reason'] = f"Model, URL'de phishing kalƒ±plarƒ± tespit etti"
                    explanation['risk_factors'] = [
                        f"URL uzunluƒüu ≈ü√ºpheli ({len(url)} karakter)",
                        f"Domain yapƒ±sƒ± riskli: {domain}",
                        "Phishing veri setindeki kalƒ±plarla benzerlik"
                    ]
                else:
                    explanation['decision_reason'] = f"Model, URL'yi g√ºvenli olarak sƒ±nƒ±flandƒ±rdƒ±"
                    explanation['confidence_factors'] = [
                        f"Normal URL yapƒ±sƒ± ({len(url)} karakter)",
                        f"G√ºvenilir domain yapƒ±sƒ±: {domain}",
                        "Bilinen g√ºvenli kalƒ±plarla benzerlik"
                    ]
            
            elif model_name == 'cybersecurity_model':
                # Cybersecurity Model a√ßƒ±klamasƒ±
                if is_phishing:
                    explanation['decision_reason'] = f"Siber g√ºvenlik analizi threat tespit etti"
                    explanation['risk_factors'] = [
                        "VirusTotal benzeri taramada ≈ü√ºpheli",
                        "K√∂t√º ama√ßlƒ± kategori tespiti",
                        "D√º≈ü√ºk g√ºvenlik puanƒ±"
                    ]
                else:
                    explanation['decision_reason'] = f"Siber g√ºvenlik analizi temiz √ßƒ±ktƒ±"
                    explanation['confidence_factors'] = [
                        "VirusTotal benzeri taramada temiz",
                        "G√ºvenli kategori tespiti",
                        "Y√ºksek g√ºvenlik puanƒ±"
                    ]
            
            elif model_name == 'phishing_urls_model':
                # Advanced URL Model a√ßƒ±klamasƒ±
                if is_phishing:
                    explanation['decision_reason'] = f"Geli≈ümi≈ü URL analizi phishing tespit etti"
                    explanation['risk_factors'] = [
                        "URL yapƒ±sƒ±nda anormallik",
                        "≈û√ºpheli parametreler",
                        "Phishing URL kalƒ±plarƒ±"
                    ]
                else:
                    explanation['decision_reason'] = f"Geli≈ümi≈ü URL analizi g√ºvenli"
                    explanation['confidence_factors'] = [
                        "Normal URL yapƒ±sƒ±",
                        "G√ºvenli parametreler",
                        "Legitimte URL kalƒ±plarƒ±"
                    ]
            
            elif model_name == 'website_model':
                # Website Feature Model a√ßƒ±klamasƒ±
                if is_phishing:
                    explanation['decision_reason'] = f"Website √∂zellik analizi riskli"
                    explanation['risk_factors'] = [
                        "≈û√ºpheli website yapƒ±sƒ±",
                        "Phishing website kalƒ±plarƒ±",
                        "Riskli domain √∂zellikleri"
                    ]
                else:
                    explanation['decision_reason'] = f"Website √∂zellik analizi g√ºvenli"
                    explanation['confidence_factors'] = [
                        "Normal website yapƒ±sƒ±",
                        "G√ºvenilir domain √∂zellikleri",
                        "Legitimte website kalƒ±plarƒ±"
                    ]
            
            elif model_name == 'crypto_scam_model':
                # Cryptocurrency Scam Model a√ßƒ±klamasƒ±
                if is_phishing:
                    explanation['decision_reason'] = f"Kripto dolandƒ±rƒ±cƒ±lƒ±k tespiti"
                    explanation['risk_factors'] = [
                        "Kripto scam kalƒ±plarƒ±",
                        "Sahte kripto sitesi √∂zellikleri",
                        "Dolandƒ±rƒ±cƒ±lƒ±k g√∂stergeleri"
                    ]
                else:
                    explanation['decision_reason'] = f"Kripto dolandƒ±rƒ±cƒ±lƒ±k tespiti temiz"
                    explanation['confidence_factors'] = [
                        "Normal kripto site yapƒ±sƒ±",
                        "G√ºvenilir kripto √∂zellikleri",
                        "Scam olmayan kalƒ±plar"
                    ]
            
            elif model_name == 'link_phishing_model':
                # Link Phishing Model a√ßƒ±klamasƒ±
                if is_phishing:
                    explanation['decision_reason'] = f"Link phishing analizi pozitif"
                    explanation['risk_factors'] = [
                        "K√∂t√º ama√ßlƒ± link kalƒ±plarƒ±",
                        "Phishing link √∂zellikleri",
                        "≈û√ºpheli y√∂nlendirme"
                    ]
                else:
                    explanation['decision_reason'] = f"Link phishing analizi negatif"
                    explanation['confidence_factors'] = [
                        "G√ºvenilir link yapƒ±sƒ±",
                        "Normal y√∂nlendirme",
                        "Safe link kalƒ±plarƒ±"
                    ]
            
            elif model_name == 'malicious_urls_model':
                # Malicious URLs Model a√ßƒ±klamasƒ±
                if is_phishing:
                    explanation['decision_reason'] = f"K√∂t√º ama√ßlƒ± URL tespiti"
                    explanation['risk_factors'] = [
                        "Malware/phishing/defacement kalƒ±plarƒ±",
                        "Multi-threat analizi pozitif",
                        "≈û√ºpheli URL davranƒ±≈üƒ±"
                    ]
                else:
                    explanation['decision_reason'] = f"K√∂t√º ama√ßlƒ± URL tespiti negatif"
                    explanation['confidence_factors'] = [
                        "Temiz URL analizi",
                        "Multi-threat tarama temiz",
                        "G√ºvenli URL davranƒ±≈üƒ±"
                    ]
            
            # Confidence d√ºzeyine g√∂re ek a√ßƒ±klama
            if confidence > 0.9:
                explanation['confidence_factors'].append(f"√áok y√ºksek g√ºven (%{confidence*100:.1f})")
            elif confidence > 0.8:
                explanation['confidence_factors'].append(f"Y√ºksek g√ºven (%{confidence*100:.1f})")
            elif confidence > 0.7:
                explanation['confidence_factors'].append(f"Orta g√ºven (%{confidence*100:.1f})")
            else:
                explanation['confidence_factors'].append(f"D√º≈ü√ºk g√ºven (%{confidence*100:.1f})")
            
            return explanation
            
        except Exception as e:
            return {
                'decision_reason': f"Model analizi tamamlandƒ±",
                'key_features': [],
                'risk_factors': [],
                'confidence_factors': [f"Confidence: %{confidence*100:.1f}"]
            }

# Global ensemble instance
ensemble_detector = None

def initialize_ensemble():
    """Ensemble detector'ƒ± ba≈ülat"""
    global ensemble_detector
    
    if ensemble_detector is None:
        print("üöÄ Ensemble Phishing Detector ba≈ülatƒ±lƒ±yor...")
        ensemble_detector = EnsemblePhishingDetector()
        print("‚úÖ Ensemble sistem hazƒ±r!")
    
    return ensemble_detector

def analyze_url_with_ensemble(url: str) -> Dict:
    """URL'yi 7-model ensemble ile analiz et"""
    
    # Global instance kullan - t√ºm 7 model y√ºkl√º
    detector = initialize_ensemble()
    
    return detector.analyze_url_comprehensive(url)

def submit_ensemble_feedback(url: str, user_feedback: str, comment: str = "") -> Dict:
    """Ensemble sistemine geri bildirim g√∂nder"""
    
    detector = initialize_ensemble()
    
    # Get last prediction for this URL
    # (In practice, you'd store predictions and link them to feedback)
    prediction_result = detector.predict_ensemble(url)
    
    feedback = {
        'url': url,
        'user_feedback': user_feedback,
        'comment': comment,
        'model_predictions': prediction_result.get('model_predictions', {}),
        'user_label': 1 if user_feedback.lower() == 'phishing' else 0,
        'ensemble_prediction': prediction_result.get('final_prediction', 0),
        'confidence': prediction_result.get('confidence', 0.0)
    }
    
    detector.save_feedback(feedback)
    
    return {
        'status': 'success',
        'message': 'Feedback saved and model weights updated',
        'updated_weights': detector.model_weights
    }

def api_analyze_url_7_models(url: str) -> Dict:
    """
    API i√ßin √∂zel 7-model ensemble analizi
    Bu fonksiyon cache sorunlarƒ±nƒ± √∂nlemek i√ßin her seferinde fresh instance olu≈üturur
    """
    print(f"üéØ API 7-MODEL ENSEMBLE ANALƒ∞Zƒ∞: {url}")
    
    try:
        # Fresh instance olu≈ütur - cache sorunu yok
        detector = EnsemblePhishingDetector()
        
        # Kapsamlƒ± analiz yap
        result = detector.analyze_url_comprehensive(url)
        
        # API formatƒ±nda d√∂nd√ºr
        api_response = {
            "ensemble_prediction": result['final_label'],
            "ensemble_confidence": result['confidence'],
            "total_models": result['total_models'],
            "active_models": result['active_models'],
            "phishing_votes": result['phishing_votes'],
            "safe_votes": result['safe_votes'],
            "voting_ratio": result['voting_ratio'],
            "model_weights": result['model_weights'],
            "rule_based_flags_count": len(result['rule_analysis'].get('flags', [])),
            "hybrid_risk_score": result['probability_phishing'],
            "ensemble_status": "success_7_models",
            "individual_models": {
                name: {
                    "prediction": pred['prediction_label'],
                    "confidence": pred['confidence']
                }
                for name, pred in result.get('model_predictions', {}).items()
            },
            "rule_analysis": result['rule_analysis'],
            "timestamp": result['timestamp'],
            "url": url,
            "final_prediction": result['final_prediction'],
            "final_label": result['final_label'],
            "probability_safe": result['probability_safe'],
            "probability_phishing": result['probability_phishing']
        }
        
        print(f"‚úÖ API Response: {api_response['active_models']}/7 model aktif")
        return api_response
        
    except Exception as e:
        print(f"‚ùå API 7-model ensemble hatasƒ±: {e}")
        return {
            "error": f"7-model ensemble hatasƒ±: {str(e)}",
            "ensemble_status": "error",
            "active_models": 0,
            "total_models": 7
        }

if __name__ == "__main__":
    # Test the ensemble system
    detector = initialize_ensemble()
    
    test_urls = [
        "https://google.com",
        "http://phishing-site-example.malicious.com/secure/verify",
        "https://facebook.com",
        "http://bit.ly/suspicious-link"
    ]
    
    print("\nüß™ ENSEMBLE SYSTEM TEST")
    print("="*50)
    
    for url in test_urls:
        print(f"\nüìù Testing: {url}")
        result = detector.analyze_url_comprehensive(url)
        
        print(f"üéØ Result: {result['final_label']} ({result['confidence']:.3f})")
        print(f"üìä Votes: {result['phishing_votes']} phishing, {result['safe_votes']} safe")
        print(f"‚ö†Ô∏è Risk factors: {len(result['analysis_details']['risk_factors'])}")
        
        for recommendation in result['analysis_details']['recommendations']:
            print(f"   {recommendation}") 