import kagglehub
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import os

def download_and_analyze_cybersecurity_dataset():
    """Cybersecurity veri setini indir ve analiz et"""
    
    print("ğŸ”„ Cybersecurity veri seti indiriliyor...")
    
    # Download latest version
    path = kagglehub.dataset_download("macisalvsalv/cybersecurity-dataset")
    print(f"ğŸ“ Path to dataset files: {path}")
    
    # KlasÃ¶rdeki dosyalarÄ± listele
    dataset_path = Path(path)
    files = list(dataset_path.glob("*"))
    print(f"\nğŸ“‹ Bulunan dosyalar:")
    for file in files:
        print(f"   - {file.name} ({file.stat().st_size / 1024 / 1024:.2f} MB)")
    
    # CSV dosyalarÄ±nÄ± bul
    csv_files = list(dataset_path.glob("*.csv"))
    
    if not csv_files:
        print("âŒ CSV dosyasÄ± bulunamadÄ±!")
        return None
    
    # Ä°lk CSV dosyasÄ±nÄ± yÃ¼kle
    main_csv = csv_files[0]
    print(f"\nğŸ“Š Ana dosya analiz ediliyor: {main_csv.name}")
    
    # Veri setini yÃ¼kle
    try:
        df = pd.read_csv(main_csv)
        print(f"âœ… Veri seti baÅŸarÄ±yla yÃ¼klendi: {df.shape}")
        
        # Temel analiz
        analyze_dataset(df, "Cybersecurity Dataset")
        
        # Veri setini proje klasÃ¶rÃ¼ne kopyala
        local_path = f"cybersecurity_dataset.csv"
        df.to_csv(local_path, index=False)
        print(f"ğŸ’¾ Veri seti yerel olarak kaydedildi: {local_path}")
        
        return df
        
    except Exception as e:
        print(f"âŒ Veri seti yÃ¼klenirken hata: {e}")
        return None

def analyze_dataset(df, dataset_name):
    """Veri setini detaylÄ± analiz et"""
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š {dataset_name.upper()} ANALÄ°ZÄ°")
    print(f"{'='*60}")
    
    # Temel bilgiler
    print(f"\nğŸ” TEMEL BÄ°LGÄ°LER:")
    print(f"   SatÄ±r sayÄ±sÄ±: {df.shape[0]:,}")
    print(f"   SÃ¼tun sayÄ±sÄ±: {df.shape[1]:,}")
    print(f"   Eksik deÄŸer sayÄ±sÄ±: {df.isnull().sum().sum():,}")
    print(f"   Bellek kullanÄ±mÄ±: {df.memory_usage().sum() / 1024 / 1024:.2f} MB")
    
    # SÃ¼tun isimleri ve tipleri
    print(f"\nğŸ“‹ SÃœTUNLAR ({len(df.columns)} adet):")
    for idx, col in enumerate(df.columns, 1):
        null_count = df[col].isnull().sum()
        dtype = df[col].dtype
        unique_count = df[col].nunique()
        print(f"   {idx:2d}. {col:25s} | {str(dtype):10s} | Null: {null_count:6,} | Unique: {unique_count:6,}")
    
    # Ä°lk birkaÃ§ satÄ±rÄ± gÃ¶ster
    print(f"\nğŸ‘€ Ä°LK 3 SATIR:")
    print(df.head(3).to_string())
    
    # Hedef deÄŸiÅŸkenini tespit et
    potential_targets = ['label', 'target', 'class', 'y', 'prediction', 'result', 'output', 'category', 'type']
    target_column = None
    
    for col in df.columns:
        col_lower = col.lower()
        if any(target in col_lower for target in potential_targets):
            target_column = col
            break
    
    if target_column is None:
        # En az unique deÄŸere sahip sÃ¼tunu hedef olarak varsay
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        if len(categorical_cols) > 0:
            unique_counts = {col: df[col].nunique() for col in categorical_cols}
            target_column = min(unique_counts, key=unique_counts.get)
    
    if target_column:
        print(f"\nğŸ¯ HEDEF DEÄÄ°ÅKEN: {target_column}")
        value_counts = df[target_column].value_counts()
        print("   DaÄŸÄ±lÄ±m:")
        for value, count in value_counts.items():
            percentage = (count / len(df)) * 100
            print(f"     {value}: {count:,} ({percentage:.1f}%)")
        
        # Dengesizlik kontrolÃ¼
        if len(value_counts) == 2:
            ratio = value_counts.max() / value_counts.min()
            if ratio > 1.5:
                print(f"   âš ï¸ SÄ±nÄ±f dengesizliÄŸi var! Oran: {ratio:.2f}:1")
            else:
                print(f"   âœ… SÄ±nÄ±flar dengeli! Oran: {ratio:.2f}:1")
    
    # SayÄ±sal sÃ¼tunlar analizi
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        print(f"\nğŸ”¢ SAYISAL SÃœTUNLAR ({len(numeric_cols)} adet):")
        print(df[numeric_cols].describe().round(2).to_string())
    
    # Kategorik sÃ¼tunlar analizi
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns
    if len(categorical_cols) > 0:
        print(f"\nğŸ“ KATEGORÄ°K SÃœTUNLAR ({len(categorical_cols)} adet):")
        for col in categorical_cols:
            unique_count = df[col].nunique()
            print(f"   {col}: {unique_count} unique deÄŸer")
            if unique_count <= 10:
                print(f"     DeÄŸerler: {list(df[col].unique())}")
    
    # Korelasyon analizi (sayÄ±sal sÃ¼tunlar iÃ§in)
    if len(numeric_cols) > 1:
        print(f"\nğŸ”— KORELASYON ANALÄ°ZÄ°:")
        correlation_matrix = df[numeric_cols].corr()
        
        # En yÃ¼ksek korelasyonlarÄ± bul
        correlation_pairs = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                col1 = correlation_matrix.columns[i]
                col2 = correlation_matrix.columns[j]
                corr_value = correlation_matrix.iloc[i, j]
                if abs(corr_value) > 0.5:  # YÃ¼ksek korelasyon
                    correlation_pairs.append((col1, col2, corr_value))
        
        if correlation_pairs:
            correlation_pairs.sort(key=lambda x: abs(x[2]), reverse=True)
            print("   YÃ¼ksek korelasyonlar (>0.5):")
            for col1, col2, corr in correlation_pairs[:5]:
                print(f"     {col1} - {col2}: {corr:.3f}")
        else:
            print("   YÃ¼ksek korelasyon bulunamadÄ±")
    
    return target_column

def suggest_algorithms(df, target_column):
    """Veri setine uygun algoritmalarÄ± Ã¶ner"""
    
    print(f"\nğŸ¤– ALGORITHM Ã–NERÄ°LERÄ°:")
    print("="*40)
    
    n_samples = len(df)
    n_features = len(df.columns) - 1 if target_column else len(df.columns)
    
    # Hedef deÄŸiÅŸken analizi
    if target_column:
        unique_targets = df[target_column].nunique()
        is_binary = unique_targets == 2
        is_balanced = True
        
        if is_binary:
            value_counts = df[target_column].value_counts()
            ratio = value_counts.max() / value_counts.min()
            is_balanced = ratio <= 1.5
    else:
        is_binary = None
        is_balanced = None
    
    suggestions = []
    
    # Veri boyutuna gÃ¶re Ã¶neriler
    if n_samples < 1000:
        suggestions.append("ğŸ”¸ **Naive Bayes** - Az veri iÃ§in ideal")
        suggestions.append("ğŸ”¸ **SVM** - KÃ¼Ã§Ã¼k veri setleri iÃ§in etkili")
        suggestions.append("ğŸ”¸ **Logistic Regression** - Basit ve etkili")
    elif n_samples < 10000:
        suggestions.append("ğŸ”¸ **Random Forest** - Orta boyut iÃ§in mÃ¼kemmel")
        suggestions.append("ğŸ”¸ **XGBoost** - YÃ¼ksek performans")
        suggestions.append("ğŸ”¸ **LightGBM** - HÄ±zlÄ± ve etkili")
    else:
        suggestions.append("ğŸ”¸ **LightGBM** - BÃ¼yÃ¼k veri iÃ§in optimize")
        suggestions.append("ğŸ”¸ **CatBoost** - Kategorik veriler iÃ§in")
        suggestions.append("ğŸ”¸ **XGBoost** - Ensemble learning")
    
    # Ã–zellik sayÄ±sÄ±na gÃ¶re
    if n_features > 100:
        suggestions.append("ğŸ”¸ **Feature Selection** gerekli olabilir")
        suggestions.append("ğŸ”¸ **Dimensionality Reduction** (PCA)")
    
    # Dengesizlik durumuna gÃ¶re
    if not is_balanced:
        suggestions.append("ğŸ”¸ **SMOTE** - Dengesizlik iÃ§in")
        suggestions.append("ğŸ”¸ **Class Weight** ayarlarÄ±")
        suggestions.append("ğŸ”¸ **Ensemble Methods** dengesizlik iÃ§in")
    
    # Veri tiplerine gÃ¶re
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns
    if len(categorical_cols) > 0:
        suggestions.append("ğŸ”¸ **CatBoost** - Kategorik veriler iÃ§in Ã¶zel")
        suggestions.append("ğŸ”¸ **Target Encoding** kategorik veriler iÃ§in")
    
    print("\nğŸ“‹ En Uygun Algoritmalar:")
    for suggestion in suggestions:
        print(f"   {suggestion}")
    
    # Ã–nerilen algoritma sÄ±ralamasÄ±
    recommended_algorithms = []
    
    if is_binary:
        if n_samples < 10000:
            recommended_algorithms = ['RandomForest', 'XGBoost', 'LightGBM', 'SVM', 'LogisticRegression']
        else:
            recommended_algorithms = ['LightGBM', 'XGBoost', 'CatBoost', 'RandomForest']
    else:
        recommended_algorithms = ['RandomForest', 'XGBoost', 'LightGBM']
    
    print(f"\nğŸ† Ã–NERÄ°LEN SIRA:")
    for i, algo in enumerate(recommended_algorithms, 1):
        print(f"   {i}. {algo}")
    
    return recommended_algorithms

if __name__ == "__main__":
    # Veri setini indir ve analiz et
    df = download_and_analyze_cybersecurity_dataset()
    
    if df is not None:
        # Hedef deÄŸiÅŸkeni bul
        potential_targets = ['label', 'target', 'class', 'y', 'prediction', 'result', 'output', 'category', 'type']
        target_column = None
        
        for col in df.columns:
            col_lower = col.lower()
            if any(target in col_lower for target in potential_targets):
                target_column = col
                break
        
        # Algoritma Ã¶nerilerini al
        recommended_algorithms = suggest_algorithms(df, target_column)
        
        print(f"\nâœ… Analiz tamamlandÄ±!")
        print(f"ğŸ“ Veri seti: cybersecurity_dataset.csv olarak kaydedildi")
        print(f"ğŸ¯ Hedef deÄŸiÅŸken: {target_column}")
        print(f"ğŸ¤– Ã–nerilen algoritma: {recommended_algorithms[0]}")
    else:
        print("âŒ Veri seti analizi baÅŸarÄ±sÄ±z!") 